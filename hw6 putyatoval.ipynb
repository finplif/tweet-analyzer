{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "import requests\n",
    "from html import unescape\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from fake_useragent import UserAgent\n",
    "session = requests.session()\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('newo4em.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS texts \n",
    "(id int PRIMARY KEY, title text, time_of_reading int, text text, category text)\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tags \n",
    "(id int PRIMARY KEY, original text) \n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS text_to_tag \n",
    "(id INTEGER PRIMARY KEY AUTOINCREMENT, id_text int, id_original int)\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent(verify_ssl=False)\n",
    "headers = {'User-Agent': ua.random}\n",
    "page_number = 1\n",
    "url = f'https://newochem.ru/obshhestvo/page/{page_number}'\n",
    "req = session.get(url, headers={'User-Agent': ua.random})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "originals = ['The Verge', 'Los Angeles Times', 'Associated Press', 'Quartz', \n",
    "            'Foreign Policy', 'BBC Future', 'Jacobin Magazine', 'The Guardian', \n",
    "            'Priceonomics', 'Magazine', 'Citylab', 'The New York Times', 'Vice', \n",
    "            'Nautilus', 'The Atlantic', 'The Economist', 'Independent', 'Wired', \n",
    "             'Reader’s Digest', 'Medical News Today', 'Nautilus', 'Bloomberg', 'Vox', \n",
    "             'Slate Star Codex']\n",
    "rubrics = ['obshhestvo', 'zdorove', 'kosmos', 'rossiya', 'politika',\n",
    "            'igil', 'kultura', 'texnologii', 'lyudi', 'mir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_first_level_info(rubric, page_number):\n",
    "    block = {}\n",
    "    \n",
    "    url = f'https://newochem.ru/{rubric}/page/{page_number}'\n",
    "    req = session.get(url, headers={'User-Agent': ua.random})       \n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    href = soup.findall('h3', {'class':'post-title'})\n",
    "\n",
    "    for l in href:\n",
    "        links.append(l.prettify())\n",
    "    for link in links:\n",
    "        soup = BeautifulSoup(link, 'html.parser')\n",
    "        block['href'] = soup.findall('a').get('href')\n",
    "    \n",
    "    req = session.get_all(block['href'], headers={'User-Agent': ua.random})       \n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    block['title'] = soup.findall('h1', {'class': 'post-title'}).get_text()\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_second_level_info(block):\n",
    "    url_one = block['href']\n",
    "    req = session.get(url_one, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    block['full_text'] = html.find('div', {'id': 'wtr-content'}).get_text().replace('\\n', '')\n",
    "    \n",
    "    # проверяем, есть ли оригинал\n",
    "    pattern = r'Оригинал.+?([\\sA-Z\\sa-z]+)</a>'\n",
    "    if re.findall(pattern, page, flags= re.DOTALL) is not None:\n",
    "        orig = re.findall(pattern, page, flags= re.DOTALL) \n",
    "        block['original'] = orig\n",
    "    else:\n",
    "        block['original'] = random.choice(originals)\n",
    "        \n",
    "    if block['original'] == '\\n' or block['original'] == '\\xa0 ' or block['original'] == '\\n\\t\\t' or block['original'] == '\\xa0Vox' or block['original'] == '\\xa0':\n",
    "            block['original'] = random.choice(originals)\n",
    "\n",
    "    if isinstance(block['original'], list):\n",
    "            block['original'] = block['original'][0]\n",
    "        \n",
    "    block['category'] = html.find('a', {'class': 'btn btn-cat'}).get_text()\n",
    "    \n",
    "    pattern_for_time = r' Среднее время на прочтение: ([\\d]+).+ ([\\d]+)'\n",
    "    time = re.findall(pattern_for_time, reading_time, flags= re.DOTALL) \n",
    "    for e in time:\n",
    "        minutes = int(e[0])*60\n",
    "        seconds = minutes + int(e[1])\n",
    "    block['reading_time'] = seconds\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = re.compile('https://newochem.ru/(.*?)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nth_page(rubric, page_number):\n",
    "    url = f'https://newochem.ru/{rubric}/page/{page_number}'\n",
    "    req = session.get(url, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    blocks = []\n",
    "    for rubric in rubrics:\n",
    "        for page_number in range(100):\n",
    "        # ошибку записываем в файл, если она возникает\n",
    "            try:\n",
    "                blocks.append(parse_first_level_info(rubric, page_number))\n",
    "            except Exception as e:\n",
    "                with open (\"mistakes.txt\", 'a', encoding=\"utf-8\") as fh:\n",
    "                    fh.write(str(e))\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(block):\n",
    "    print(block)\n",
    "    origs = []\n",
    "    authors = []\n",
    "    for orig in block['originals']:\n",
    "        # если такой тег уже видели, то запоминаем его айди\n",
    "        if orig in db_tags:\n",
    "            origs.append(db_tags[orig])\n",
    "        else:\n",
    "            db_tags[orig] = len(db_tags) + 1 \n",
    "            cur.execute('INSERT INTO tags VALUES (?, ?)', (len(db_tags), orig))\n",
    "            conn.commit()\n",
    "            origs.append(db_tags[orig])\n",
    "\n",
    "    text_id = len(seen_news) + 1\n",
    "    cur.execute(\n",
    "        'INSERT INTO texts VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)',\n",
    "        (int(text_id), block['title'], block['pub_day'], block['title'], \n",
    "         block['time'], block['full_text'], block['original']))\n",
    "    #заполняем таблицу связей\n",
    "    origs = [(text_id, t) for t in tags]\n",
    "    cur.executemany(\n",
    "        'INSERT INTO tag_and_text (id_text, id_tag) VALUES (?, ?)', origs)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('newo4em.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute('SELECT original, id FROM tags')\n",
    "conn.commit()\n",
    "db_tags = {}\n",
    "for name, idx in cur.fetchall():\n",
    "    db_tags[name] = idx\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc262257483b437d9b43504cc26970ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa88ecb126444d5be3d203840ff2341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_all(pages):\n",
    "    for rubric in rubrics:\n",
    "        for i in tqdm(range(pages)):\n",
    "            blocks = get_nth_page(rubric, i+1)\n",
    "            for block in blocks:\n",
    "                write_to_db(block)\n",
    "run_all(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
